<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Francisco Requena</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Francisco Requena</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 28 Jun 2018 00:00:00 +0100</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Francisco Requena</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An introduction to uncertainty with Bayesian models</title>
      <link>/post/introduction-uncertainty-bayesian-models/</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/introduction-uncertainty-bayesian-models/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;In this post, we will get a first approximation to the “uncertainty” concept. First, we will train two models: logistic regression and its “Bayesian version” and compare their performance. Furthermore, we will explore the advantage of using a Bayesian model when we want to estimate how likely is our prediction. Finally, we will briefly discuss why there are some predicted values more probable than others.&lt;/p&gt;
&lt;div id=&#34;get-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get the data&lt;/h2&gt;
&lt;p&gt;First, we download this data from &lt;a href=&#34;https://www.kaggle.com/gilsousa/habermans-survival-data-set&#34;&gt;Kaggle&lt;/a&gt;. This dataset includes 306 patients from a study of patients that had undergone a surgical operation on breast cancer. The table consists of three explanatory variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Age of patient during surgical operation (&lt;code&gt;age&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Year when the operation was made (&lt;code&gt;operation_year&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Number of positive axillary nodes detected (&lt;code&gt;nodes&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, there is a column (&lt;code&gt;survival&lt;/code&gt;) that indicates whether the patient survived at least 5 years after the operation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork) # merge plots
library(ggridges) # ridges plot
library(glue) # paste plot labels
library(yardstick) # helper roc curves and auc
library(rstanarm) # bayesian model
library(bayestestR) # helper for the bayesian model
library(broom) # make tidy

# Source: https://www.kaggle.com/gilsousa/habermans-survival-data-set
haberman &amp;lt;- read_csv(&amp;#39;data/haberman.csv&amp;#39;, col_names = c(&amp;#39;age&amp;#39;, 
                                                        &amp;#39;operation_year&amp;#39;, 
                                                        &amp;#39;nodes&amp;#39;, 
                                                        &amp;#39;survival&amp;#39;))

haberman &amp;lt;- haberman %&amp;gt;%
  mutate(survival = factor(if_else(survival == 1, &amp;#39;Yes&amp;#39;, &amp;#39;No&amp;#39;))) %&amp;gt;%
  mutate(operation_year = factor(operation_year)) %&amp;gt;%
  mutate(id = as.character(row_number())) %&amp;gt;% 
  select(id, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory analysis&lt;/h2&gt;
&lt;p&gt;Since the dataset has 3 explanatory variables, let’s plot the distribution of each one of them with the response variable &lt;em&gt;survival&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- haberman %&amp;gt;%
  ggplot(aes(age)) +
  geom_density(aes(fill = survival), color = &amp;#39;black&amp;#39;, alpha = 0.4) +
  theme_bw()

p2 &amp;lt;- haberman %&amp;gt;%
  ggplot(aes(nodes)) +
  geom_density(aes(fill = survival), color = &amp;#39;black&amp;#39;, alpha = 0.4) +
  theme_bw()

p3 &amp;lt;- haberman %&amp;gt;%
  group_by(operation_year, survival) %&amp;gt;%
  summarise(n = n()) %&amp;gt;%
  mutate(perc = 100*(n / sum(n))) %&amp;gt;%
  ggplot(aes(operation_year, perc)) +
  geom_col(aes(fill = survival), color = &amp;#39;black&amp;#39;) +
  theme_bw() +
  labs(y = &amp;#39;Percentage (%)&amp;#39;)


p1 + p2 + p3 + patchwork::plot_layout(nrow = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Age and number of nodes seem to have a reasonable distribution but surprisingly, patient survival does not increase along the operation year. In theory, the patient survival of most cancer types has increased dramatically over the years. Therefore, it seems reasonable to find a similar pattern in this dataset. The interval of time (1958-1969) seems long enough and happened during a period of major progress in clinical therapies.&lt;/p&gt;
&lt;p&gt;A plausible explanation is an underlying effect of, at least, one remaining variable. Let’s observe the distribution of the variable age of patient over the years:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;haberman %&amp;gt;%
  ggplot(aes(age, operation_year)) +
  geom_density_ridges(aes(fill = operation_year), show.legend = FALSE) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;haberman %&amp;gt;%
  ggplot(aes(operation_year, age)) +
  geom_boxplot(aes(fill = operation_year), show.legend = FALSE) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There seem to be differences over the years. In this post, further analysis to control for this effect is out of scope, but a more exhaustive analysis of this dataset should be aware of it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;classification&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Classification&lt;/h1&gt;
&lt;p&gt;Once we have explored quickly the dataset, we are going to train a model to try to predict whether the patient survived 5 years after the operation.&lt;/p&gt;
&lt;p&gt;To do so, we are going to test two different approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Logistic regression&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bayesian generalized linear models&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this blog post, we will skip aspects such as cross-validation, feature engineering, precision-recall curve, or unbalanced labels (there is).&lt;/p&gt;
&lt;div id=&#34;split-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split data&lt;/h2&gt;
&lt;p&gt;First, we will split the available dataset &lt;code&gt;haberman&lt;/code&gt; into two sets, a training (70%) and a test (30%).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(991)

training_ids &amp;lt;- haberman %&amp;gt;% sample_frac(0.7) %&amp;gt;% pull(id)

hab_training &amp;lt;- haberman %&amp;gt;% 
  filter(id %in% training_ids) %&amp;gt;% 
  mutate(operation_year = as.integer(operation_year))

hab_test &amp;lt;- haberman %&amp;gt;% 
  filter(!id %in% training_ids) %&amp;gt;% 
  mutate(operation_year = as.integer(operation_year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will train independently both models with the training set and predict the labels of the response variable (“survive”, “no survive”) in the test dataset. These predicted labels will be useful to compare both models in terms of performance and further aspects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training---logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training - logistic regression&lt;/h2&gt;
&lt;p&gt;In R, we just need to use the &lt;code&gt;glm&lt;/code&gt; function and specify the argument &lt;code&gt;family = binomial&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_model &amp;lt;- glm(survival ~ age + nodes + operation_year, family = &amp;#39;binomial&amp;#39;, 
                      data = hab_training)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;training---bayesian-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training - Bayesian logistic regression&lt;/h2&gt;
&lt;p&gt;Thanks to the package &lt;code&gt;rstanarm&lt;/code&gt; that provides an elegant interface to &lt;a href=&#34;https://mc-stan.org/&#34;&gt;stan&lt;/a&gt;, we can keep almost the same syntax used before. In this case, we use the function &lt;code&gt;stan_glm&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bayesian_model &amp;lt;- rstanarm::stan_glm(survival ~ age + nodes + operation_year, 
                                     family = &amp;#39;binomial&amp;#39;,
                                     data = hab_training,
                                     prior = normal())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.127 seconds (Warm-up)
## Chain 1:                0.137 seconds (Sampling)
## Chain 1:                0.264 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.128 seconds (Warm-up)
## Chain 2:                0.139 seconds (Sampling)
## Chain 2:                0.267 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.129 seconds (Warm-up)
## Chain 3:                0.141 seconds (Sampling)
## Chain 3:                0.27 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.117 seconds (Warm-up)
## Chain 4:                0.146 seconds (Sampling)
## Chain 4:                0.263 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;performance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance&lt;/h2&gt;
&lt;p&gt;Once we trained both models, we are going to compare their performance with the test set (split at the beginning of the post). To that end, we calculate the ROC curve and the Area Under the Curve (AUC) of each model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_logistic &amp;lt;- predict(logistic_model, hab_test, type = &amp;#39;response&amp;#39;)
pred_bayesian &amp;lt;- posterior_linpred(bayesian_model, newdata = hab_test, transform = TRUE) %&amp;gt;% 
  as_tibble() %&amp;gt;%
  map_dbl(~ map_estimate(.x) )

check_pred &amp;lt;- hab_test %&amp;gt;%
  select(id) %&amp;gt;%
  mutate(pred_surv_no_log = pred_logistic,
         pred_surv_no_bay = 1-pred_bayesian) %&amp;gt;%
  left_join(haberman %&amp;gt;% select(id, survival), by = &amp;#39;id&amp;#39;)


roc_logistic &amp;lt;- check_pred %&amp;gt;% roc_curve(survival, pred_surv_no_log) %&amp;gt;% mutate(model = &amp;#39;logistic&amp;#39;)
roc_bayesian &amp;lt;- check_pred %&amp;gt;% roc_curve(survival, pred_surv_no_bay) %&amp;gt;% mutate(model = &amp;#39;bayesian&amp;#39;)

auc_logistic &amp;lt;- check_pred %&amp;gt;% roc_auc(survival, pred_surv_no_log) %&amp;gt;% pull(.estimate) %&amp;gt;% round(3)
auc_bayesian &amp;lt;- check_pred %&amp;gt;% roc_auc(survival, pred_surv_no_bay) %&amp;gt;% pull(.estimate) %&amp;gt;% round(3)

roc_both &amp;lt;- roc_logistic %&amp;gt;% bind_rows(roc_bayesian)


roc_both %&amp;gt;%
  ggplot(aes((1-specificity), sensitivity)) +
    geom_line(aes(color = model), size = 1) +
    theme_bw() +
  geom_abline(linetype = 3) +
    labs(title = &amp;#39;Comparison performance logistic and Bayesian model&amp;#39;,
         subtitle = glue(&amp;#39;AUC (logistic) = {auc_logistic} - AUC (Bayesian) = {auc_bayesian}&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both models demonstrate similar performance. If we would have to decide, at this step of the analysis, one of them (logistic or Bayesian), there would not be a reason to choose one or the other. Probably, the logistic one, since it may sounds more familiar. But this might change when the &lt;strong&gt;uncertainty idea&lt;/strong&gt; comes up!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;uncertainty&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Uncertainty&lt;/h2&gt;
&lt;p&gt;First, we are going to explore the outcomes of the test set provided by the &lt;em&gt;logistic model&lt;/em&gt;. These values represent the probability [2] of each instance of being labeled as “No survive” five years after the operation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_logistic &amp;lt;- predict(logistic_model, hab_test, type = &amp;#39;response&amp;#39;)

p1 &amp;lt;- pred_logistic %&amp;gt;%
  enframe() %&amp;gt;%
  ggplot(aes(value)) +
  geom_density(fill = &amp;#39;steelblue&amp;#39;, alpha = 0.5) +
  theme_bw() +
  labs(x = &amp;#39;Probability&amp;#39;, y = &amp;#39;Density&amp;#39;)


p2 &amp;lt;- pred_logistic %&amp;gt;%
  enframe() %&amp;gt;%
  ggplot(aes(value)) +
  geom_histogram(fill = &amp;#39;yellow&amp;#39;, alpha = 0.5, color = &amp;#39;black&amp;#39;, binwidth = 0.05) +
  theme_bw() +
  labs(x = &amp;#39;Probability&amp;#39;, y = &amp;#39;Density&amp;#39;)

p1 + p2 + plot_layout(ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each probability value represents a single observation. To convert the predicted probability to labels, the user needs to specify a threshold where every value above the threshold is defined as “No survive”, otherwise “survive”. Most of the cases, this creates problematic scenarios where two observations can be equally labeled in spite of having distinct probabilities (e.g. 0.6 and 0.95).&lt;/p&gt;
&lt;p&gt;By contrast, for each one of observations in the test set, the &lt;strong&gt;Bayesian model&lt;/strong&gt; does not provide a single probability value but a posterior distribution. We can represent the posterior distributions from the 92 observations (test set) with a boxplot, for instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_uncertainty &amp;lt;- posterior_linpred(bayesian_model, newdata = hab_test, transform = TRUE) %&amp;gt;% 
  as_tibble() %&amp;gt;%
  pivot_longer(everything(), names_to = &amp;#39;rank_obs&amp;#39;, values_to = &amp;#39;pred_surv_n_bay&amp;#39;)

plot_uncertainty %&amp;gt;%
  ggplot(aes(rank_obs, pred_surv_n_bay)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = &amp;#39;Test set - Observation&amp;#39;, y = &amp;#39;Probability (survival == &amp;quot;No survive&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way to check the dispersion of the predicted outcome is with a ridge plot, that is especially useful when the number of samples is low. So, let’s pick only two observations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_uncertainty %&amp;gt;%
  # if you want to reproduce this code, just change the character &amp;#39;1&amp;#39; or &amp;#39;5&amp;#39; for any other.
  filter(rank_obs %in% c(&amp;#39;1&amp;#39;, &amp;#39;5&amp;#39;)) %&amp;gt;%
  ggplot(aes(pred_surv_n_bay, rank_obs)) +
    geom_density_ridges(aes(fill = rank_obs), alpha = 0.6) +
    theme_bw() +
    labs(y = &amp;#39;Test set - Observation&amp;#39;, x = &amp;#39;Probability (survival == &amp;quot;No survive&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 0.0101&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we observe that both distributions have a “peak” (known as MAP [1]) above 0.5, therefore the predicted label would be, in both cases, ‘no survive’. But, are these two predictions equally certain? Well, we notice, at least, two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Both MAPs have different values (sample 1 - 0.9, sample 5 - 7.2).&lt;/li&gt;
&lt;li&gt;Observation number 5 has a flatter curve in comparison with the number 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In both cases, observation 5 reflects a higher uncertainty regarding its predicted label in comparison with observation 1. Should we make the same clinical decisions in both cases? Would this information be valuable in a clinical environment…? Probably yes, but first, we should find a way to measure it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measuring-uncertainty-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Measuring uncertainty [3]&lt;/h2&gt;
&lt;p&gt;A handy option is to use the standard deviation (sd) of the distribution, so we can estimate one value for each observation. With this in mind, we can plot the distribution of the sd from the 92 observations of the test dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;std_dev_tbl &amp;lt;- plot_uncertainty %&amp;gt;%
  group_by(rank_obs) %&amp;gt;%
  summarise(std_dev = sd(pred_surv_n_bay)) %&amp;gt;%
  ungroup()


std_dev_tbl %&amp;gt;% 
  ggplot(aes(std_dev)) +
  geom_density(fill = &amp;#39;steelblue&amp;#39;, alpha = 0.5) +
  theme_bw() +
  labs(x = &amp;#39;Standard deviation (sd)&amp;#39;,
       y = &amp;#39;Density&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of observations have a standard deviation of around 0.04. There are a few extreme values in the interval 0.08-0.10. In short, this plot shows that there are some observations whose sd is twice as high as others.&lt;/p&gt;
&lt;p&gt;We can filter and select observations based on the dispersion of its posterior distribution. For instance, we can split the test set of 92 observations in percentiles using the sd and plot the 1st (lowest sd) and 10th percentile (highest sd). In this way, it allows us to compare those observations with the highest and lowest standard deviation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_sd &amp;lt;- std_dev_tbl %&amp;gt;% 
  mutate(tile = ntile(std_dev, 10)) %&amp;gt;%
  filter(tile == 1 | tile == 10)

plot_uncertainty %&amp;gt;%
  # left_join(std_dev, by = &amp;#39;rank_obs&amp;#39;) %&amp;gt;%
  filter(rank_obs %in% top_sd$rank_obs) %&amp;gt;%
  ggplot(aes(pred_surv_n_bay, rank_obs)) +
  geom_density_ridges(aes(fill = rank_obs), alpha = 0.6, show.legend = FALSE) +
  theme_bw() +
  labs(y = &amp;#39;Test set - Observation&amp;#39;, x = &amp;#39;Probability (survival == &amp;quot;No survive&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we easily identify to which group each observation belongs to. Independently of the predicted labels, should their predictions be considered equally likely? If the final user of the model just receives a categorical outcome, he/she is definitely skipping some valuable information since some predictions look more unlikely than others. As an alternative, predictions could be grouped into categories and neglect those with a high dispersion or make it clear than further support should be required.&lt;/p&gt;
&lt;p&gt;In this post, we have measured the uncertainty of observations and identifying those samples with high uncertainty. But, we have not talked yet about what is the origin of it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-some-predictions-are-more-unlikely-than-others&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why some predictions are more unlikely than others?&lt;/h2&gt;
&lt;p&gt;In other words, why our model has more doubts about a sample than others? I find two possible explanations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The sample is mislabeled.&lt;/li&gt;
&lt;li&gt;Group variability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first one is difficult to address but we can explore the group variability. Since we have three continuous explanatory variable, we can easily do a PCA with the function &lt;code&gt;prcomp&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_tbl &amp;lt;- hab_test %&amp;gt;%
  # take only numeric columns
  select_if(is.numeric) %&amp;gt;%
  # Important - we need to scale and center each variable before PCA
  prcomp(scale = TRUE, center = TRUE) %&amp;gt;%
  tidy() %&amp;gt;%
  mutate(row = as.character(row)) %&amp;gt;%
  pivot_wider(id_cols = row, values_from = value, names_from = PC, names_prefix = &amp;#39;PC&amp;#39;) %&amp;gt;%
  left_join(top_sd %&amp;gt;% select(-std_dev), by = c(&amp;#39;row&amp;#39; = &amp;#39;rank_obs&amp;#39;)) %&amp;gt;%
  mutate(tile = ifelse(is.na(tile), &amp;#39;ok&amp;#39;, tile)) %&amp;gt;%
  left_join(hab_test %&amp;gt;% select(survival) %&amp;gt;% mutate(row = as.character(row_number())), 
            by = &amp;#39;row&amp;#39;)


pca_tbl %&amp;gt;%
  ggplot(aes(PC1, PC2)) +
    geom_point() +
    theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we have observations in two categories, let’s split them into two plots:&lt;/p&gt;
&lt;p&gt;Since we have observations defined into two categories (survival = Yes, survival = No), let’s lay out the plot into two different:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_tbl %&amp;gt;%
  ggplot(aes(PC1, PC2)) +
    geom_point() +
    theme_bw() +
    facet_grid(~ survival)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, we are going to highlight those observations that belong to the highest and lowest sd groups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_tbl %&amp;gt;%
  mutate(tile = case_when(
    tile == &amp;#39;1&amp;#39; ~ &amp;#39;lowest sd&amp;#39;,
    tile == &amp;#39;10&amp;#39; ~ &amp;#39;highest sd&amp;#39;,
    tile == &amp;#39;ok&amp;#39; ~ &amp;#39;ok&amp;#39;
  )) %&amp;gt;%
  ggplot(aes(PC1, PC2)) +
    geom_point(aes(fill = tile), color = &amp;#39;black&amp;#39;, shape = 21, size = 2) +
    theme_bw() +
    facet_grid(~ survival) +
    labs(fill = &amp;#39;Category&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the one hand, “lowest sd” group observations are centrally located in the plot. This reflects a tendency of these samples to have similar features values with observations belonging to their own label. On the other hand, “highest sd” group points tend to be dispersed from the rest, all over the components. It makes sense since the uncertainty to predict these points come from the fact that their own feature values are different from points on the same category.&lt;/p&gt;
&lt;p&gt;Surprisingly, there is a red point on the left panel whose location is centric respect to the rest of the values. This perhaps arises the disadvantage of reducing a probability distribution to a point-estimate (standard deviation). The dispersion estimation might have not be accurate enough and further ways of measuring might be needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As humans, we make decisions based on uncertainty, even though we are not aware of it. If the weather forecasters show 10% of raining on the weekend, we will probably make a plan to go to the mountain. With 90% we may rethink about it…When we are talking with someone about a delicate topic, we pick the words based on the uncertainty of his/her predicted response: words with a broad meaning and therefore ambiguous might not be chosen, due to the high uncertainty. Therefore, if we constantly map our reality and &lt;em&gt;act&lt;/em&gt; through the constant evaluation of uncertainty, &lt;em&gt;why should we believe in predictions from machines without a shadow of doubt?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;My personal view is that the measurement of uncertainty will end up being essential. Especially, for every decision process supported by a machine in a clinical environment. In that way, I find &lt;em&gt;Bayesian models&lt;/em&gt; a nice fit for many of the challenges of tomorrow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;[1] To calculate the roc curves of the Bayesian model’s predictions, a single probability value for each observation is required. There are multiple ways to estimate it, such as mean, median, and MAP (Highest Maximum A Posteriori). In this case, I chose the latest because it provided the highest performance.&lt;/p&gt;
&lt;p&gt;[2] The function &lt;code&gt;predict&lt;/code&gt; retrieves outcomes as probabilities because we specified &lt;code&gt;type = response&lt;/code&gt; as argument. Otherwise, the default output would be as logit.&lt;/p&gt;
&lt;p&gt;[3] I am interested to know more ways to estimate the “uncertainty” of a prediction. Please if you have any reference or idea, let me know! ;P&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Poisson distribution applied in genomics</title>
      <link>/post/poisson-distribution-applied-in-genomics/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/poisson-distribution-applied-in-genomics/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post, I will discuss briefly what is the Poisson distribution and describe two examples extracted from research articles in the genomics field. One of them based on the distribution of structural variants across the genome and other about &lt;em&gt;de novo&lt;/em&gt; variants in a patient cohort.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson distribution&lt;/h2&gt;
&lt;p&gt;In genomics, many of the events we observe correspond to countable values. For instance, the number of mutations found in a specific type of genomic regions or a patient cohort, sequence reads…The Poisson distribution is a &lt;em&gt;discrete probability model&lt;/em&gt; that takes countable numbers as the mentioned before and will be defined as &lt;em&gt;events&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To calculate a Poisson distribution, we need to specify a single parameter which is called lambda (&lt;span class=&#34;math display&#34;&gt;\[\lambda\]&lt;/span&gt;). This value is known as the &lt;em&gt;rate parameter&lt;/em&gt; and defines the mean number of events in a given interval. In other words, if we know the total number of events of our system, we just need to divide it by the number of intervals. We will see further some examples of this.&lt;/p&gt;
&lt;p&gt;Once we know lambda, we can calculate the probability of seeing &lt;span class=&#34;math display&#34;&gt;\[/x\]&lt;/span&gt; number of events on a given interval, following this formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P\left( x \right) = \frac{{e^{ - \lambda } \lambda ^x }}{{x!}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the next example, we are going to generate a Poisson distribution of 100 samples whose lambda value is equal to 2 with the &lt;code&gt;rpois&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries
library(dplyr)
library(ggplot2)
library(gganimate)
library(tidyr)


tibble(x = rpois(n = 100, lambda = 2)) %&amp;gt;%
 ggplot(aes(x)) +
 geom_histogram(binwidth = 1, fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;) + 
 theme_bw() +
 labs(title = &amp;#39;Poisson distribution&amp;#39;, x = &amp;#39;Events&amp;#39;, y = &amp;#39;Count&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-14-poisson-genomics_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An interesting aspect about the Poisson distribution: the mean and variance of the distribution are equal to the value of lambda. Therefore, the probability of finding an interval with 3 events (for instance) is higher as long as we increase the value of lambda.&lt;/p&gt;
&lt;p&gt;Confusing? Check out this example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;poisson_tbl &amp;lt;- tibble(lambda = seq(1.2, 7, 0.2)) %&amp;gt;% 
 rowwise() %&amp;gt;%
 mutate(value = paste(rpois(1000, lambda), collapse = &amp;#39;,&amp;#39;)) %&amp;gt;%
 separate_rows(value, sep = &amp;#39;,&amp;#39;) %&amp;gt;%
 mutate(value = as.integer(value)) %&amp;gt;%
 mutate(prob_7 = round(dpois(7, lambda), 5))

poisson_tbl %&amp;gt;%
 ggplot(aes(value)) +
 geom_histogram(binwidth = 1, fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;) +
 transition_states(lambda) +
 geom_vline(xintercept = 7, color = &amp;#39;red&amp;#39;, linetype = 4 ) +
 labs(title = &amp;#39;Poisson distribution&amp;#39;, 
  subtitle = &amp;#39;lambda: {closest_state}, P(X = 7) : {poisson_tbl[poisson_tbl$lambda == {closest_state},] %&amp;gt;% pull(prob_7) %&amp;gt;% unique()}&amp;#39;,
  x = &amp;#39;Events&amp;#39;,
  y = &amp;#39;Count&amp;#39;) +
 theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-14-poisson-genomics_files/figure-html/unnamed-chunk-2-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;In the example above, we are generating 30 times a set of 1000 random values following a Poisson distribution, increasing each time the value of lambda (from 1.2 to 7). The probability of finding an interval with 7 events (red line) is higher as long as we increase lambda.&lt;/p&gt;
&lt;p&gt;As we said before, we only need lambda to generate a Poisson distribution. Generally, we calculate this value if we know beforehand the number of events and intervals (as we will see in the second example, this is not always the case).&lt;/p&gt;
&lt;p&gt;Let’s put some examples of what an interval or event can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Intervals&lt;/em&gt;. We can define intervals as &lt;em&gt;fixed&lt;/em&gt; time units, such as days, months, years…or also delimited areas of a geographical region (see this nice &lt;a href=&#34;https://sciprincess.wordpress.com/2019/03/05/cancer-clusters-and-the-poisson-distributions/&#34;&gt;post&lt;/a&gt; of cancer clusters or &lt;a href=&#34;https://www.wired.com/2012/12/what-does-randomness-look-like/&#34;&gt;this one&lt;/a&gt; about the distribution of impacts of V-1 and V-2 missiles during WWII).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Events&lt;/em&gt;. The amount of clicks on a banner or the number of homicides or blackouts every year…An important condition is that each event is independent of each other (events occur independently).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Every time, we perform a Poisson distribution, we always ask ourselves the same question: are these events distributed randomly across the intervals?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-distribution-in-genomics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson distribution in genomics&lt;/h2&gt;
&lt;p&gt;In genomics, as many other fields, there are different ways to define intervals and events. In the next examples, we will explore two completely different approaches:&lt;/p&gt;
&lt;div id=&#34;example-1-structural-variants-in-the-human-genome&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;em&gt;Example 1&lt;/em&gt; : Structural variants in the human genome&lt;/h3&gt;
&lt;p&gt;Structural Variants (SVs) are mutations of more than 50bp and include deletions, duplications, inversion, translocations…These types of variants are important causes of multiple disorders, such as autism, schizoprenia, autoimmune diseases or developmental disorders.&lt;/p&gt;
&lt;p&gt;In this article &lt;a href=&#34;https://academic.oup.com/gbe/article/11/4/1136/5384496&#34;&gt;Fine-scale characterization of genomic structural variation in the human genome reveals adaptive and biomedically relevant hotspots&lt;/a&gt; [1], the authors explore whether the distribution of this type of mutations is random or follow any pattern across the genome.&lt;/p&gt;
&lt;p&gt;To do this, the researchers defined each structural variant as an event. Next, they divided the human genome into 100 kb intervals and after discarding incomplete intervals (the intervals need to be &lt;em&gt;fixed&lt;/em&gt;), they got a total of 28,103 intervals.&lt;/p&gt;
&lt;p&gt;The number of structural variants is 42,758 SVs. Therefore, to calculate lambda, they just had to divide this number by the total number of intervals. Finally, they generated a Poisson distribution and defined as “hotspot regions” all the intervals that exceeded the 99th percentile (6 SVs per 100 kb interval) concluding that these intervals had more SVs than expected by chance. Furthermore, they were able to identify “desert regions” as those intervals with a lower nº of SVs as compared with the number expected by chance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-de-novo-variants-in-neurodevelopmental-disorders&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;em&gt;Example 2&lt;/em&gt; : De novo variants in neurodevelopmental disorders&lt;/h3&gt;
&lt;p&gt;In this article &lt;a href=&#34;https://www.nature.com/articles/nature25983&#34;&gt;De novo mutations in regulatory elements in neurodevelopmental disorders&lt;/a&gt; [2], the researchers explore the impact
of de novo variants (those present on children but not their parents) on regulatory regions of the genome in a cohort of patients with neurodevelopmental disorders. The majority of patients in this cohort did not present any de novo mutations (DNMs) in protein-coding genes. Therefore, a plausible hypothesis is to find some of these DNMs in those regions of the DNA yet unexplored: regulatory regions.&lt;/p&gt;
&lt;p&gt;Most of the human genome (98%) do not encode for protein regions. Therefore, the researchers decided to narrow down the search and focus only on those regulatory regions based on two features: regions highly conserved or experimentally validated.&lt;/p&gt;
&lt;p&gt;Finally, they found DNMs mapping this set of regulatory regions, which is great since it allows us to identify the causal mutation and find a diagnos….but wait a minute: Each person’s genome harbors many variants and most of the time, these variants are not harmful. So, we expect to find variants &lt;em&gt;randomly&lt;/em&gt; in these regulatory regions just &lt;em&gt;by chance&lt;/em&gt;. Yes, as you can guess…here it comes the &lt;em&gt;Poisson distribution&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The researchers knew this fact, therefore, to validate their results, they performed (surprise…) a Poisson distribution. First, they calculated the lambda parameter following the next approach:&lt;/p&gt;
&lt;p&gt;They focused on 6,239 individuals and counted the number of mutations found in regulatory regions. Furthermore, for each region, they calculated the expected number of mutations given the nucleotide context.&lt;/p&gt;
&lt;p&gt;Once they got the expected number of mutations for each regulatory region, they summed the values and multiplied by the total number of individuals (6,239) to obtain lambda. This value represents the expected number of mutations. Finally, they generated a Poisson distribution with lambda and compared it with the number of observed mutations. This allowed them to demonstrate, first, there were some subgroups of regulatory regions with an excess of the novo variants and second, this excess could be considered as statistically significative. These significant regions were mostly featured by fetal brain DNase signal.&lt;/p&gt;
&lt;p&gt;Contrary to what we saw at the beginning of the post, the approach to calculate lambda has been completely different in the second example. Precisely, this versatility makes the Poisson distribution one of the most popular ways to model counted data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We discussed here about two different scenarios whose events were defined as mutations. But the Poisson distribution can help us to modelate other kind of events, for instance, sequence data. One of the most used techniques for the identification of peaks in Chip-seq analysis is called Model-based Analysis of ChIP-Seq data ( &lt;a href=&#34;https://genomebiology.biomedcentral.com/articles/10.1186/gb-2008-9-9-r137&#34;&gt;MACS&lt;/a&gt;). This program generates a Poisson distribution to identify regions with a higher number of reads than just by chance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When we use the genome to produce fixed size intervals to generate a Poisson distribution, an important aspect, it is the genome size. In principle, we already know this value: ~3,100 milions b.p (hg19) and ~ 3,200 millions b.p (hg38). Unfortunately, there are many inaccesible regions (gap regions) represented by Ns. Therefore, the use of the total size would artificially decrease the value of lambda and increase the number of false findings. As a consequence, we need to provide a &lt;em&gt;effective genome size&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Lin, Yen-Lung, and Omer Gokcumen. “Fine-scale characterization of genomic structural variation in the human genome reveals adaptive and biomedically relevant hotspots” Genome biology and evolution 11.4 (2019): 1136-1151.&lt;/p&gt;
&lt;p&gt;[2] Short, Patrick J., et al. “De novo mutations in regulatory elements in neurodevelopmental disorders.” Nature 555.7698 (2018): 611-616.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimating pi value with Monte Carlo simulation</title>
      <link>/post/estimating-pi-value-with-monte-carlo-simulation/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/estimating-pi-value-with-monte-carlo-simulation/</guid>
      <description> 



&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load of libraries

library(tidyverse)
library(sp)
library(gganimate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_simulations &amp;lt;- 3000

df &amp;lt;- tibble(
  
  values_x = runif(n_simulations,0,1),
  values_y = runif(n_simulations,0,1)
)


circleFun &amp;lt;- function(center=c(0,0), diameter=1, npoints=100, start=0, end=2)
{
  tt &amp;lt;- seq(start*pi, end*pi, length.out=npoints)
  data.frame(x = center[1] + diameter / 2 * cos(tt), 
             y = center[2] + diameter / 2 * sin(tt))
}

dat &amp;lt;- circleFun(c(0,0), 2, start=1.5, end=2.5)


df &amp;lt;- df %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(label = point.in.polygon(values_x, values_y, dat$x, dat$y, mode.checked=FALSE)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(count_in = cumsum(label),
         id = row_number(),
         pi_value = 4*(count_in / id))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df) +
  geom_rect(aes(xmin = -1, xmax = 1, ymin = -1, ymax = 1), 
            colour = &amp;quot;black&amp;quot;, show.legend = FALSE) +
  geom_polygon(aes(x, y), data = dat, alpha = 0.4) +
  geom_point(aes(x = values_x, y = values_y, fill = factor(label), group=id), 
             color = &amp;#39;black&amp;#39;, shape = 21, show.legend = FALSE) +
  theme_minimal() +
  coord_cartesian(ylim=c(0, 1), xlim = c(0,1)) +
  transition_reveal(id)  +
  labs(title = &amp;#39;Nº of observations: {frame_along} of {n_simulations}&amp;#39;,
       subtitle = &amp;#39;Estimated value of pi: {df$pi_value[as.integer(frame_along)]}&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-03-estimating-pi-value-with-monte-carlo-simulation_files/figure-html/unnamed-chunk-2-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df, aes(id, pi_value)) +
  geom_line() +
  geom_point(colour = &amp;#39;red&amp;#39;, size = 3) +
  transition_reveal(id) +
  geom_hline(yintercept = pi, color = &amp;#39;red&amp;#39;, linetype= &amp;#39;dashed&amp;#39;) +
  theme_bw() +
  labs(title = &amp;#39;Nº of observations: {frame_along} of {n_simulations}&amp;#39;,
       subtitle = &amp;#39;Estimated value of pi: {df$pi_value[as.integer(frame_along)]}&amp;#39;,
       x = &amp;#39;Sample size&amp;#39;,
       y = &amp;#39;Pi value&amp;#39;) +
    coord_cartesian(ylim=c(0, 4)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-03-estimating-pi-value-with-monte-carlo-simulation_files/figure-html/unnamed-chunk-3-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring world flights using a network approach</title>
      <link>/post/exploring-world-airline-network/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-world-airline-network/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, I started to read &lt;a href=&#34;http://networksciencebook.com&#34;&gt;this free accessible book&lt;/a&gt; written by Albert-László Barabási. In the Chapter 4 of his book, it depicts the USA airport networks to &lt;strong&gt;represent scale-free networks&lt;/strong&gt;. I was wondering if we can get a &lt;em&gt;world picture&lt;/em&gt;, creating the same network but including the global routes using open data from internet.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-a-scale-free-network&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. What is a scale-free network?&lt;/h2&gt;
&lt;p&gt;Scale-free networks are characterized by a large number of nodes with low degree (number of links) and very few hubs with a high degree. If we represent the distribution of degrees of these nodes, it follows a power-law distribution. To illustrate this idea, let’s create a quick example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries
library(tidygraph) 
library(ggraph)
library(igraph)
library(stringr)
library(tidyverse)
library(patchwork)
library(ggthemes)

# 1. Example showing a scale-free network

scale_free_net &amp;lt;- play_barabasi_albert(n = 1000, power = 1)

# 1.1 Scale-free network
p1 &amp;lt;- ggraph(scale_free_net, layout = &amp;#39;kk&amp;#39;) + 
  geom_edge_link(alpha = 0.3) + 
  geom_node_point(fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;, shape = 21) +
  ggtitle(&amp;#39;Scale-free network&amp;#39;) + 
  theme_graph()

vector_values &amp;lt;- degree_distribution(scale_free_net)[-1] # Eliminate first element, it represents zero degree vertices
  
df &amp;lt;- data.frame(frequency = vector_values,
                 degrees = seq(1, length(vector_values),1))

# 1.2 Degree distribution
p2 &amp;lt;- ggplot(df, aes(degrees, frequency)) +
  geom_col(fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;) +
  ggtitle(&amp;#39;Degree Distribution of a scale-free network&amp;#39;) +
  ylab(&amp;#39;Relative frequency&amp;#39;) +
  xlab(&amp;#39;Number of links&amp;#39;) +
  theme_bw()

p1 + p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Many real networks share this feature. For instance, if we take a look how internet is organized and calculate the number of links that every site has, we find that the most of websites (nodes) have a low number of links (edges) and very few will have a large number of links (e.g. Google, Facebook…). Other examples are social, co-authorship or protein-protein network.&lt;/p&gt;
&lt;p&gt;We hope to see the same pattern through our airport’s network: &lt;em&gt;very few airports have a large number of routes while the most will have few routes&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-airports-and-routes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Data (airports and routes)&lt;/h2&gt;
&lt;p&gt;At the beginning, we talked about creating our own network of airlines routes. To achieve this, we download our data from &lt;a href=&#34;https://openflights.org&#34;&gt;Openflights&lt;/a&gt; whose have a lot of information about flights. We will just download data about &lt;strong&gt;airports&lt;/strong&gt; (selecting: code, longitude and latitude) and &lt;strong&gt;routes&lt;/strong&gt; (selecting: name, code source, code destination and continent location). Besides, we will clean those observations with NA’s values or wrong strings.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Aiports will be the nodes of our network and the routes will conform the edges between the nodes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data with routes

# https://openflights.org/data.html#route

df &amp;lt;- read.csv(&amp;#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat&amp;#39;,header = FALSE,
               stringsAsFactors = FALSE,
               col.names = c(&amp;#39;airline&amp;#39;, &amp;#39;airline_id&amp;#39;, &amp;#39;src&amp;#39;, &amp;#39;src_id&amp;#39;, &amp;#39;dest&amp;#39;, &amp;#39;dest_id&amp;#39;, &amp;#39;codeshare&amp;#39;,&amp;#39;stops&amp;#39;,  &amp;#39;equip&amp;#39;))[,c(3,5)]

# Data with airport information

# https://openflights.org/data.html#airport
df2 &amp;lt;- read.csv(&amp;#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat&amp;#39;,header = FALSE,
                   stringsAsFactors = FALSE)[,c(2,5,7,8,12)]
colnames(df2) &amp;lt;- c(&amp;#39;name&amp;#39;,&amp;#39;code&amp;#39;, &amp;#39;lat&amp;#39;, &amp;#39;long&amp;#39;, &amp;#39;location&amp;#39;)


# Clean data

df_airport &amp;lt;- df2 %&amp;gt;% 
  filter(!str_detect(code, fixed(&amp;quot;\\N&amp;quot;))) %&amp;gt;%
  filter(!str_detect(location, fixed(&amp;quot;\\N&amp;quot;))) %&amp;gt;%
  as_tibble()

tmp_loc &amp;lt;- str_split(df_airport$location, &amp;#39;/&amp;#39;)
df_airport$location &amp;lt;- map_chr(tmp_loc, function(x) x[[1]])
df_airport &amp;lt;- df_airport %&amp;gt;% mutate(location = as.factor(location))

df_routes &amp;lt;- df %&amp;gt;% 
  filter(!str_detect(src, fixed(&amp;quot;\\N&amp;quot;)) &amp;amp; !str_detect(dest, fixed(&amp;quot;\\N&amp;quot;))) %&amp;gt;%
  filter(!src == dest) %&amp;gt;%
  group_by(src, dest) %&amp;gt;%
  count() %&amp;gt;%
  arrange(desc(n)) %&amp;gt;%
  ungroup() %&amp;gt;%
  as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;airport-network-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Airport Network visualization&lt;/h2&gt;
&lt;p&gt;To make possible the downstream analysis, we have to transform the observations of our dataframe into nodes and edges (tbl_graph object). We can do this thanks to the &lt;strong&gt;package ggraph&lt;/strong&gt;. Once we do this, we will be able to visualise the network applying different algorithms layers and calculate topological parameters of the nodes that otherwise would not be possible.&lt;/p&gt;
&lt;p&gt;For instance, we can choose the ‘mds’ layout (you can find many other layouts described &lt;a href=&#34;https://www.data-imaginist.com/2017/ggraph-introduction-layouts/&#34;&gt;here&lt;/a&gt;). This algorithm layout measures the shortest path between each node and display together those nodes which are closer in the network. Besides, we are going to calculate some scores per node and to make faster the algorithm, I will eliminate those airports whose number of routes are low.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Convert dataframe (df_routes) to tbl_graph object (df_graph)
df_graph &amp;lt;- as_tbl_graph(df_routes,directed = FALSE) %&amp;gt;% activate(edges) %&amp;gt;%
  filter(!edge_is_multiple()) %&amp;gt;% activate(nodes) %&amp;gt;%
  mutate(n_degree = centrality_degree(),
         betweenness = centrality_betweenness(),
         community = group_walktrap(),
         n_triangles = local_triangles(),
         clust = local_transitivity()) %&amp;gt;%
  left_join(df_airport, by = c(&amp;#39;name&amp;#39; = &amp;#39;code&amp;#39;)) %&amp;gt;%
  filter(!is.na(lat) &amp;amp; !is.na(long))

# ggraph(df_graph %&amp;gt;% activate(nodes) %&amp;gt;% filter(n_degree &amp;gt;= 10), layout = &amp;quot;mds&amp;quot;) + 
#   geom_edge_link(aes(edge_width = n), alpha = 0.1, edge_colour = &amp;#39;gray&amp;#39;) + 
#   geom_node_point(aes(size = n_degree, fill = location), shape = 21) +
#   scale_fill_brewer(palette = &amp;#39;Set1&amp;#39;) +
#   scale_size(range = c(0, 14)) +
#   theme_graph() +
#   guides(size=FALSE, edge_width = FALSE, fill = guide_legend(override.aes = list(size = 7))) +
#   ggtitle(&amp;#39;Airports network&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Besides, we plot the degree distribution of our network using ggplot2. For that, we convert our tbl_graph to a dataframe (the reverse step we did before) applying the function &lt;em&gt;activation(nodes)&lt;/em&gt; and then &lt;em&gt;as_tibble()&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Degree distribution
df_nodes &amp;lt;- df_graph %&amp;gt;% activate(nodes) %&amp;gt;% as_tibble()
ggplot(df_nodes, aes(n_degree)) +
  geom_histogram(fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;, binwidth = 1) +
  ggtitle(&amp;#39;Degree Distribution of airports network&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Number of links&amp;#39;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we saw at the beginning, both networks follow a power-law distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-is-my-airport&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Where is my airport?&lt;/h2&gt;
&lt;p&gt;At first glance, let’s take a look at the distribution of the airports around the world based on their region:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;worldmap &amp;lt;- borders(&amp;quot;world&amp;quot;, colour=&amp;quot;#efede1&amp;quot;, fill=&amp;quot;#efede1&amp;quot;) 

# Get airports by degree
ggplot(df_airport, aes(long, lat)) + worldmap + 
  geom_point(aes(fill = location), color = &amp;#39;black&amp;#39;, shape = 21) +
  theme_void() +
  guides(fill = guide_legend(override.aes = list(size = 7))) +
  ggtitle(&amp;#39; Aiports across the world by region&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see the biggest hubs are influenced by the economical situation and the population density of the region.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-are-the-best-connected-airports&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What are the best connected airports?&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(df_nodes %&amp;gt;% filter(n_degree &amp;gt;= 50), aes(long, lat)) + worldmap + 
  geom_point(aes(size = n_degree, fill = n_degree), pch = 21) +
  scale_fill_viridis_c() +
  theme_void() +
  scale_size_continuous(range = c(1,10))

p2 &amp;lt;- ggplot(df_nodes %&amp;gt;% top_n(20, n_degree), aes(reorder(name, -n_degree), n_degree)) +
  geom_col(aes(fill = n_degree), color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 20 airport by number of routes&amp;#39;) +
  ylab(&amp;#39;Nº of routes&amp;#39;) +
  xlab(&amp;#39;Code Airport&amp;#39;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))



p3 &amp;lt;- ggplot(df_nodes %&amp;gt;% group_by(location) %&amp;gt;% top_n(10, n_degree), aes(reorder(name, -n_degree), n_degree)) +
  geom_col(aes(fill = n_degree), color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 10 airport by number of routes and region&amp;#39;) +
  ylab(&amp;#39;Nº of routes&amp;#39;) +
  xlab(&amp;#39;Code Airport&amp;#39;) +
  facet_wrap(~ location, scales = &amp;#39;free_x&amp;#39;) +
  theme_bw() +
  guides(fill = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))

p1 + p2  + plot_layout(ncol = 1, heights = c(3, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;what-is-the-longest-path-possible&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the longest path possible?&lt;/h2&gt;
&lt;p&gt;Can you guess how many steps would be required to travel the longest path possible between two airports? This number is called diameter and can be calculated easily:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_graph %&amp;gt;% activate(nodes) %&amp;gt;% 
  mutate(diam = graph_diameter()) %&amp;gt;% 
  distinct(diam) %&amp;gt;% 
  as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##    diam
##   &amp;lt;dbl&amp;gt;
## 1    12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The longest path is 12 steps. Not so long if we take into account the remote distance of some of the airports (Siberia, Greenland, Pacific regions…).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-shortest-path-between-two-airports&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the shortest path between two airports?&lt;/h2&gt;
&lt;p&gt;We can select an airport and calculate the shortest path needed to reach another one. For instance, the Charles de Gaulle Airport (Paris) is one step from Adolfo Suárez Madrid–Barajas (Madrid), but what is the number of steps needed to reach the Hawai’s airport from Paris? Let’s calculate it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shortest_paths(df_graph, &amp;#39;CDG&amp;#39;, &amp;#39;HNL&amp;#39;)$vpath[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## + 3/3209 vertices, named, from 9a73413:
## [1] CDG ORD HNL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;shortest path&lt;/strong&gt; from Paris to Honolulu is: Paris -&amp;gt; Chicago -&amp;gt; Honolulu.&lt;/p&gt;
&lt;p&gt;Now, imagine that we calculate all the shortest paths between Paris and the rest of airports and we repeat it with every airport and calculate the average. This value is called: &lt;strong&gt;average shortest path&lt;/strong&gt; and is average number of minimum connections required from any airport to any other airport.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_graph %&amp;gt;% activate(nodes) %&amp;gt;% 
  mutate(dist = graph_mean_dist()) %&amp;gt;% 
  distinct(dist) %&amp;gt;% 
  as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##    dist
##   &amp;lt;dbl&amp;gt;
## 1  3.97&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The average shortesth path is 3.94, almost 4 steps on average to go from an airport to any other.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-longest-distance-possible-from-a-specific-airport&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the longest distance possible from a specific airport?&lt;/h2&gt;
&lt;p&gt;We are in Paris again, and we want to go to the most distant airport possible (in steps). This value is called &lt;strong&gt;eccentricity&lt;/strong&gt; and is specific for each airport. Let’s take a look at three of the most connected airports:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_graph_eccen &amp;lt;- df_graph %&amp;gt;% activate(nodes) %&amp;gt;% 
  mutate(eccentricity = node_eccentricity()) %&amp;gt;% as_tibble()
  
df_graph_eccen %&amp;gt;% 
  filter(name == &amp;#39;ATL&amp;#39; | name == &amp;#39;CDG&amp;#39; | name == &amp;#39;AMS&amp;#39;) %&amp;gt;% 
  select(name.y, eccentricity )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   name.y                                           eccentricity
##   &amp;lt;chr&amp;gt;                                                   &amp;lt;dbl&amp;gt;
## 1 Hartsfield Jackson Atlanta International Airport            7
## 2 Charles de Gaulle International Airport                     7
## 3 Amsterdam Airport Schiphol                                  7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We would need 7 steps to go from Paris to the most distant airport, the same value obtained with Atlanta and Amsterdam airports. This make sense as we have selected nodes with the highest nº of routes. But the value 7 is the lowest that we can get?&lt;/p&gt;
&lt;p&gt;Let’s see the distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The filter(eccentricity &amp;gt; 2) eliminate those airports that are disconnected from the main network and have a eccentricity from 0 to 2
ggplot(df_graph_eccen %&amp;gt;% filter(eccentricity &amp;gt; 2), aes(eccentricity)) +
  geom_histogram(fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;) +
  ylab(&amp;#39;Nº of airports&amp;#39;) +
  theme(text = element_text(size=20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we see above, most of the airports are located between 8 and 9. Those airports with the highest number of routes have a value of 7. But there is an airport whose value is 6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; df_graph_eccen %&amp;gt;% filter(eccentricity == 6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 11
##   name  n_degree betweenness community n_triangles clust name.y   lat  long
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 YYZ        147     249941.         1        2061 0.192 Leste~  43.7 -79.6
## # ... with 2 more variables: location &amp;lt;fct&amp;gt;, eccentricity &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, it is interesting that the airport with the lowest eccentricity is &lt;strong&gt;Lester B. Pearson International Airport&lt;/strong&gt; located at Toronto. Its number of routes (n_degree) is not very high but has an important particularity. If we see the map, Canada is a country with a large number of airports sparse along the territory. While the majority of airports have to “spend” steps to reach those distant airport (mainly at the north of the territory), this airport is very close to them and at the same time is close to the rest of airports across the world (USA, Europe, China…)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-are-the-hubs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where are the hubs?&lt;/h2&gt;
&lt;p&gt;We can detect also the most relevant hubs (densely connected subgraphs) and display those airports that belongs to one of the top 10 hubs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_nodes %&amp;gt;% filter(community &amp;lt;= 10), aes(long, lat)) + worldmap + 
  geom_point(aes(fill = as.factor(community)), color = &amp;#39;black&amp;#39;, shape = 21) +
  theme_void() +
  scale_fill_brewer(palette = &amp;#39;Paired&amp;#39;) +
  guides(fill = guide_legend(override.aes = list(size = 12))) +
  ggtitle(&amp;#39; Aiports across the world by region&amp;#39;) +
  labs(fill=&amp;quot;List of Hubs&amp;quot;) +
  theme_map()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have applied a walktrap community finding algorithm that uses random walks between the nodes and group those airports that are connected by short random walks.&lt;/p&gt;
&lt;p&gt;If you take a look at the map, these hubs represent not only a group of airports densely connected but also political and economical hubs. For instance, a hub includes Ex-soviets states, another Europe, Canary Islands and some cities from Magreb.&lt;/p&gt;
&lt;p&gt;In addition, we can classify the airports in 3 categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Core&lt;/strong&gt;: Those aiports whose have the highest number of triangles (subgraph of 3 nodes and 3 edges). If an airport is located in many triangles, we consider it as a well connected airport.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Peryphery&lt;/strong&gt;: Airports that are located in distant regions with few routes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bridge&lt;/strong&gt;: Those airports that allow the communication between the airports that form the core and the periphery.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_nodes &amp;lt;- df_nodes %&amp;gt;% mutate(category = &amp;#39;Bridge&amp;#39;)
df_nodes$category &amp;lt;- ifelse(df_nodes$n_triangles &amp;gt; 400, &amp;#39;Core&amp;#39;, df_nodes$category)
df_nodes$category &amp;lt;- ifelse(df_nodes$clust == 0, &amp;#39;Periphery&amp;#39;, df_nodes$category)

ggplot(df_nodes, aes(long, lat)) + worldmap +
  geom_point(aes(fill = category), color = &amp;#39;black&amp;#39;, shape = 21) +
  facet_grid(category ~.) +
  theme_map() +
  theme(strip.text = element_text(size=25)) +
  guides(fill = guide_legend(override.aes = list(size = 20)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;which-are-the-best-connected-airports&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Which are the best connected airports?&lt;/h1&gt;
&lt;p&gt;There are different ways to measure the connectivity of a node in a network. One of the most used is the betweenness centrality which is the sum of the shortest paths that pass through a node:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(df_nodes %&amp;gt;% filter(n_degree &amp;gt;= 20), aes(long, lat)) + worldmap + 
  geom_point(aes(size = betweenness, fill = betweenness), pch = 21) +
  scale_fill_viridis_c() +
  theme_void() +
  scale_size_continuous(range = c(1,10))

p2 &amp;lt;- ggplot(df_nodes %&amp;gt;% top_n(20, betweenness), aes(reorder(name, -betweenness), betweenness)) +
  geom_col(aes(fill = betweenness), color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 20 airport by number of betweenness&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Code Airport&amp;#39;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))

p3 &amp;lt;- ggplot(df_nodes %&amp;gt;% group_by(location) %&amp;gt;% top_n(10, betweenness), aes(reorder(name, -betweenness), betweenness)) +
  geom_col(aes(fill = betweenness), color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 10 airport by number of betweenness and region&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Code Airport&amp;#39;) +
  facet_wrap(~ location, scales = &amp;#39;free_x&amp;#39;) +
  theme_bw() +
  guides(fill = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))

p1 + p2 + plot_layout(ncol = 1, heights = c(3, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-15-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we see above, airports with with a high number of routes usually have a high betweenness. But we find an exception: the &lt;strong&gt;Ted Stevens Anchorage International Airport (ANL)&lt;/strong&gt;. Honestly, I did not expect this airport with the highest betweenness but if we take a look at the organization of the Alaska’s airports:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_routes_def &amp;lt;- df_routes %&amp;gt;% 
  left_join(df_airport, by = c(&amp;#39;src&amp;#39; = &amp;#39;code&amp;#39;)) %&amp;gt;%
  rename(long_src = long, lat_src = lat) %&amp;gt;%
  left_join(df_airport, by = c(&amp;#39;dest&amp;#39; = &amp;#39;code&amp;#39;)) %&amp;gt;%
  rename(long_dest = long, lat_dest = lat) %&amp;gt;%
  left_join(df_nodes, by = c(&amp;#39;src&amp;#39; = &amp;#39;name&amp;#39;)) %&amp;gt;%
  select(-lat, -long)

df_routes_anc &amp;lt;- df_routes_def %&amp;gt;% 
  filter( dest == &amp;#39;ANC&amp;#39;)
        
ggplot(df_routes_anc, aes(long_src, lat_src)) + worldmap +
  coord_map(xlim=c(-180,180)) +
            geom_segment(aes(x = long_src, y = lat_src,
                  xend = long_dest, yend = lat_dest),
               alpha = 0.7, color = &amp;#39;steelblue&amp;#39;) +
  scale_fill_viridis_c() +
  theme_map() +
  ggtitle(&amp;#39;Ted Stevens Anchorage International Airport&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Crossing this airport is required to reach the rest of airports in Alaska. Therefore, this create a bottleneck where most of nodes have to cross this airport before reach the rest.&lt;/p&gt;
&lt;div id=&#34;routes-by-number-of-airlines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;7. Routes by number of airlines&lt;/h2&gt;
&lt;p&gt;We can take a look at those routes whose have the largest number of airlines:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_routes %&amp;gt;% top_n(20, n), aes(reorder(paste(src, dest, sep =&amp;#39; - &amp;#39;), -n), n)) +
  geom_col(aes(fill = n),  color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 20 routes by number of airlines&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Route&amp;#39;) +
  theme_bw() +
  guides(fill = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;connections-between-madrid-and-dubai&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;8. Connections between Madrid and Dubai&lt;/h1&gt;
&lt;p&gt;We can display all the connections between Madrid and Dubai.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_routes_dubai &amp;lt;- df_routes_def %&amp;gt;% 
  filter(  src == &amp;#39;DXB&amp;#39; | dest == &amp;#39;DXB&amp;#39;)
        
p1 &amp;lt;- ggplot(df_routes_dubai, aes(long_src, lat_src)) + worldmap +
  coord_map(&amp;quot;gilbert&amp;quot;, xlim=c(-180,180)) +
  geom_segment(aes(x = long_src, y = lat_src,
                  xend = long_dest, yend = lat_dest),
               alpha = 0.3, color = &amp;#39;steelblue&amp;#39;) +
  scale_fill_viridis_c() +
  theme_map() +
  ggtitle(&amp;#39;Dubai International Airport connections&amp;#39;) 

p2 &amp;lt;- ggplot(df_routes_dubai %&amp;gt;% filter(src == &amp;#39;DXB&amp;#39;) %&amp;gt;% top_n(10, n) , aes(reorder(paste(src, dest, sep =&amp;#39; - &amp;#39;), -n), n)) +
  geom_col(aes(fill = n),  color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 10 routes by number of airlines&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Route&amp;#39;) +
  theme_bw() +
  guides(fill = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))


p1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;FALSE Warning: Removed 10 rows containing missing values (geom_segment).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-18-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;…or the routes between Madrid and the rest of the world:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_routes_madrid &amp;lt;- df_routes_def %&amp;gt;% 
  filter(  src == &amp;#39;MAD&amp;#39; | dest == &amp;#39;MAD&amp;#39;)
        
p1 &amp;lt;- ggplot(df_routes_madrid, aes(long_src, lat_src)) + worldmap +
  coord_map(&amp;quot;gilbert&amp;quot;, xlim=c(-180,180)) +
  geom_segment(aes(x = long_src, y = lat_src,
                  xend = long_dest, yend = lat_dest),
               alpha = 0.3, color = &amp;#39;orange&amp;#39;) +
  scale_fill_viridis_c() +
  theme_map() +
  ggtitle(&amp;#39;Madrid Barajas International Airport connections&amp;#39;)

p2 &amp;lt;- ggplot(df_routes_madrid %&amp;gt;% filter(src == &amp;#39;MAD&amp;#39;) %&amp;gt;% top_n(10, n) , aes(reorder(paste(src, dest, sep =&amp;#39; - &amp;#39;), -n), n)) +
  geom_col(aes(fill = n),  color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 10 routes by number of airlines&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Route&amp;#39;) +
  theme_bw() +
  guides(fill = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))


p1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;FALSE Warning: Removed 4 rows containing missing values (geom_segment).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-19-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;relevance of an airport&lt;/strong&gt; in the network can be assessed through different metrics: nº of routes, nº of triangles, clustering, betweenness, eccentricity or shortest path. At the same time, the &lt;strong&gt;identification of groups of airports&lt;/strong&gt;, we have clustered airports by continent, random walks algorithm, or using a blend of centrality measures filtering the nodes in three groups (core, bridge, peripherial).&lt;/p&gt;
&lt;p&gt;In conclusion, network science allows us to improve our knowledge about data that can be converted into a network, through the use of multiple approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To simplify this post, I have not included the direction of the edges neither the real distance between airports.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A very interesting point is the analysis of the resilence: what would happen if we delete a specific airport from the network? Would the impact be equal across the aiports?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Can we predict cases of dengue with climate variables?</title>
      <link>/post/can-we-predict-cases-of-dengue-with-climate-variables/</link>
      <pubDate>Sat, 09 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/post/can-we-predict-cases-of-dengue-with-climate-variables/</guid>
      <description>


&lt;p&gt;Recently, I discovered a new website about competitions that it is not called Kaggle! Its name is Drivendata.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DrivenData&lt;/strong&gt; offers different competitions related with multiple types of field, such as health (oh yes!), ecology, society… with a common element: to face the world’s biggest social challenges.&lt;/p&gt;
&lt;p&gt;I decided to join my first competition called &lt;em&gt;‘DengAI: Predicting Disease Spread‘&lt;/em&gt;. In this case, the user receives a set of weather information (temperatures, precipitations, vegetations) from two cities: &lt;strong&gt;San Juan&lt;/strong&gt; (Puerto Rico) and &lt;strong&gt;Iquitos&lt;/strong&gt; (Peru) with total cases of dengue by year and week of every year.&lt;/p&gt;
&lt;p&gt;The goal of the competition is to develop a prediction model that would be able to anticipate the cases of dengue in every city depending on a set of climate variables.&lt;/p&gt;
&lt;p&gt;The DrivenData’s blog wrote some days ago, a post about a fast approach with this dataset. It was written in Python. So, I decided to “translate” to R language.&lt;/p&gt;
&lt;p&gt;The next code is divided into three main points:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Code with clean tasks (transform NA values, remove of columns…) and exploratory analyses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Function with every step during cleaning of data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Development of model, prediction and comparison of predicted vs real total cases detected.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load of libraries

library(tidyverse)
library(zoo)
library(corrplot)
library(MASS)
library(reshape2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load data
train_features &amp;lt;- read.csv(&amp;#39;data/dengue_post/dengue_features_train.csv&amp;#39;)
  
train_labels &amp;lt;- read.csv(&amp;#39;data/dengue_post/dengue_labels_train.csv&amp;#39;)

test_features &amp;lt;- read.csv(&amp;#39;data/dengue_post/dengue_features_test.csv&amp;#39;)

submission_format &amp;lt;- read.csv(&amp;#39;data/dengue_post/submission_format.csv&amp;#39;)
  
# Filter of data by city  

sj_train_labels &amp;lt;- filter(train_labels, city == &amp;#39;sj&amp;#39;)
sj_train_features &amp;lt;- filter(train_features, city == &amp;#39;sj&amp;#39;)

iq_train_labels &amp;lt;- filter(train_labels, city == &amp;#39;iq&amp;#39;)
iq_train_features &amp;lt;- filter(train_features, city == &amp;#39;iq&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Is there NA values?

df_na_sj &amp;lt;- as.data.frame(apply(sj_train_features,2, function(x) any(is.na(x))))
colnames(df_na_sj) &amp;lt;- &amp;#39;is_there_NA&amp;#39;
df_na_sj$number_NA &amp;lt;- apply(sj_train_features,2, function(x) sum(is.na(x)))
df_na_sj$mean_NA &amp;lt;- apply(sj_train_features, 2, function(x) mean(is.na(x)))

df_na_iq &amp;lt;- as.data.frame(apply(iq_train_features, 2, function(x) any(is.na(x))))
colnames(df_na_iq) &amp;lt;- &amp;#39;is_there_NA&amp;#39;
df_na_iq$number_NA &amp;lt;- apply(iq_train_features, 2, function(x) sum(is.na(x)))
df_na_iq$mean_NA &amp;lt;- apply(iq_train_features, 2, function(x) mean(is.na(x)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Vegetation Index over Time Plot with NAs

ggplot(sj_train_features, aes(x = as.Date(week_start_date), y = ndvi_ne )) +
  ggtitle(&amp;#39;Vegetation Index over Time&amp;#39;) +
  theme_bw() +
  xlab(&amp;#39;Title&amp;#39;) +
  geom_line(na.rm = FALSE, color = &amp;#39;blue&amp;#39;) +
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove &amp;#39;weekofyear&amp;#39; column

sj_train_features &amp;lt;- dplyr::select(sj_train_features, -week_start_date)

iq_train_features &amp;lt;- dplyr::select(iq_train_features, -week_start_date)

# Fill the NA values with the previous value

sj_train_features &amp;lt;- sj_train_features %&amp;gt;%
            do(na.locf(.))

iq_train_features &amp;lt;- iq_train_features %&amp;gt;%
            do(na.locf(.))

# Distribution of labels

# print(mean(sj_train_labels$total_cases))
# print(var(sj_train_labels$total_cases))
# 
# print(mean(iq_train_labels$total_cases))
# print(var(iq_train_labels$total_cases))


ggplot(sj_train_labels, aes(x = total_cases)) +
  theme_bw() +
  ggtitle(&amp;#39;Cases of dengue in San Juan&amp;#39;) +
  geom_histogram() +
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iq_train_labels, aes(x = total_cases)) +
  theme_bw() +
  ggtitle(&amp;#39;Cases of dengue in Iquitos&amp;#39;) +
  geom_histogram() +
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Add total_cases column to *_train_features dataframes


# sj_train_features &amp;lt;- left_join(sj_train_features, sj_train_labels, by = c(&amp;#39;city&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;weekofyear&amp;#39;))
 sj_train_features$total_cases &amp;lt;- sj_train_labels$total_cases

# iq_train_features &amp;lt;- left_join(iq_train_features, iq_train_labels, by = c(&amp;#39;city&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;weekofyear&amp;#39;))
iq_train_features$total_cases &amp;lt;- iq_train_labels$total_cases

# Correlation matrix

m_sj_train_features &amp;lt;- data.matrix(sj_train_features)
m_sj_train_features &amp;lt;- cor(x = m_sj_train_features[,3:24], use = &amp;#39;complete.obs&amp;#39;, method = &amp;#39;pearson&amp;#39;)

m_iq_train_features &amp;lt;- data.matrix(iq_train_features)
m_iq_train_features &amp;lt;- cor(x = m_iq_train_features[,3:24], use = &amp;#39;everything&amp;#39;, method = &amp;#39;pearson&amp;#39;)

# Correlation Heatmap

corrplot(m_sj_train_features, type = &amp;#39;full&amp;#39;, tl.col = &amp;#39;black&amp;#39;, method=&amp;quot;shade&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrplot(m_iq_train_features, type = &amp;#39;full&amp;#39;, tl.col = &amp;#39;black&amp;#39;, method = &amp;#39;shade&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlation Bar plot

df_sj_train_features &amp;lt;- data.frame(m_sj_train_features)[2:21,] 
df_sj_train_features &amp;lt;- dplyr::select(df_sj_train_features, total_cases) 
                                    
df_iq_train_features &amp;lt;- data.frame(m_iq_train_features)[2:21,]
df_iq_train_features &amp;lt;- dplyr::select(df_iq_train_features, total_cases) 

ggplot(df_sj_train_features, aes(x= reorder(rownames(df_sj_train_features), -total_cases), y = total_cases)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;) +
  theme_bw() +
  ggtitle(&amp;#39;Correlation of variables in San Juan&amp;#39;) +
  ylab(&amp;#39;Correlation&amp;#39;) +
  xlab(&amp;#39;Variables&amp;#39;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_iq_train_features, aes(x= reorder(rownames(df_sj_train_features), -total_cases), y = total_cases)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;) +
  theme_bw() +
  ggtitle(&amp;#39;Correlation of variables in Iquitos&amp;#39;) +
  ylab(&amp;#39;Correlation&amp;#39;) +
  xlab(&amp;#39;Variables&amp;#39;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function data cleaning

data_clean &amp;lt;- function(df_dengue_features, df_dengue_labels = NULL, add_cases = TRUE) {
  
  # Filter by city
  sj_df_dengue_features &amp;lt;- filter(df_dengue_features, city == &amp;#39;sj&amp;#39;)
  iq_df_dengue_features &amp;lt;- filter(df_dengue_features, city == &amp;#39;iq&amp;#39;)
  
  if (add_cases == TRUE) {
  sj_df_dengue_labels &amp;lt;- filter(df_dengue_labels, city == &amp;#39;sj&amp;#39;)
  iq_df_dengue_labels &amp;lt;- filter(df_dengue_labels, city == &amp;#39;iq&amp;#39;)
  }
  # Removing week_start_date column
  sj_df_dengue_features &amp;lt;- dplyr::select(sj_df_dengue_features, -week_start_date)
  iq_df_dengue_features &amp;lt;- dplyr::select(iq_df_dengue_features, -week_start_date)

  # Fill of NA values with the previous value
  sj_df_dengue_features &amp;lt;- sj_df_dengue_features %&amp;gt;%
    do(na.locf(.))
  
  iq_df_dengue_features &amp;lt;- iq_df_dengue_features %&amp;gt;%
    do(na.locf(.))
  
  # Add total_cases to dataframe with features
  if (add_cases == TRUE) {
  sj_df_dengue_features$total_cases &amp;lt;- sj_df_dengue_labels$total_cases
  iq_df_dengue_features$total_cases &amp;lt;- iq_df_dengue_labels$total_cases
  }
  
  # Converting character columns into numbers
  sj_df_dengue_features &amp;lt;- as.data.frame(apply(sj_df_dengue_features,2,as.numeric))
  sj_df_dengue_features$city &amp;lt;- rep(&amp;#39;sj&amp;#39;, nrow(sj_df_dengue_features))
  iq_df_dengue_features &amp;lt;- as.data.frame(apply(iq_df_dengue_features,2,as.numeric))
  iq_df_dengue_features$city &amp;lt;- rep(&amp;#39;iq&amp;#39;, nrow(iq_df_dengue_features))
  
  result &amp;lt;- list(sj_df_dengue_features, iq_df_dengue_features )
  
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Getting data_training clean

data_train &amp;lt;- data_clean(train_features, train_labels, TRUE)

# Getting negative binomials models by city

training_sj &amp;lt;- glm.nb(formula = total_cases ~ reanalysis_specific_humidity_g_per_kg +
                     reanalysis_dew_point_temp_k +
                     station_min_temp_c +
                     station_avg_temp_c, data = data_train[[1]])

training_iq &amp;lt;- glm.nb(formula = total_cases ~ reanalysis_specific_humidity_g_per_kg +
                        reanalysis_dew_point_temp_k +
                        station_min_temp_c +
                        station_avg_temp_c, data = data_train[[2]])

# Getting data_test clean

data_test &amp;lt;- data_clean(test_features, add_cases = FALSE)


# Testing model with training data

prediction_train_sj &amp;lt;-  predict(training_sj, data_train[[1]], type = &amp;#39;response&amp;#39;)
prediction_train_iq &amp;lt;-  predict(training_iq, data_train[[2]], type = &amp;#39;response&amp;#39;)

df_prediction_train_sj &amp;lt;- data.frame(&amp;#39;prediction&amp;#39; = prediction_train_sj, &amp;#39;actual&amp;#39; = data_train[[1]]$total_cases,
                                     &amp;#39;time&amp;#39; = as.Date(train_features$week_start_date[1:936]))
df_prediction_train_sj &amp;lt;- melt(df_prediction_train_sj, id.vars = &amp;#39;time&amp;#39;)
ggplot(df_prediction_train_sj, aes(x = time, y = value, color = variable)) +
  geom_line() +
  ggtitle(&amp;#39;Dengue predicted Cases vs. Actual Cases (City-San Juan) &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_prediction_train_iq &amp;lt;- data.frame(&amp;#39;prediction&amp;#39; = prediction_train_iq, &amp;#39;actual&amp;#39; = data_train[[2]]$total_cases,
                                     &amp;#39;time&amp;#39; = as.Date(train_features$week_start_date[937:1456]))
df_prediction_train_iq &amp;lt;- melt(df_prediction_train_iq, id.vars = &amp;#39;time&amp;#39;)
ggplot(df_prediction_train_iq, aes(x = time, y = value, color = variable)) +
  geom_line() +
  ggtitle(&amp;#39;Dengue predicted Cases vs. Actual Cases (City-Iquitos) &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prediction of total_cases in the data set

prediction_sj &amp;lt;-  predict(training_sj, data_test[[1]], type = &amp;#39;response&amp;#39;)
prediction_iq &amp;lt;-  predict(training_iq, data_test[[2]], type = &amp;#39;response&amp;#39;)
 
data_prediction_sj &amp;lt;- data.frame(&amp;#39;city&amp;#39; = rep(&amp;#39;sj&amp;#39;, length(prediction_sj) ), 
                                 &amp;#39;total_cases&amp;#39; = prediction_sj, 
                                 &amp;#39;weekofyear&amp;#39; = data_test[[1]]$weekofyear,
                                 &amp;#39;year&amp;#39; = data_test[[1]]$year )

data_prediction_iq &amp;lt;- data.frame(&amp;#39;city&amp;#39; = rep(&amp;#39;iq&amp;#39;, length(prediction_iq) ), 
                                 &amp;#39;total_cases&amp;#39; = prediction_iq,
                                 &amp;#39;weekofyear&amp;#39; = data_test[[2]]$weekofyear,
                                 &amp;#39;year&amp;#39; = data_test[[2]]$year)


  
submission_format$total_cases &amp;lt;- as.numeric(c(data_prediction_sj$total_cases, 
                                                   data_prediction_iq$total_cases))

submission_format$total_cases &amp;lt;- round(submission_format$total_cases, 0)
  
write.csv(submission_format,
          file = &amp;#39;submission_format_submit.csv&amp;#39;, row.names = F)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Drugsplot</title>
      <link>/project/drugsplot/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/drugsplot/</guid>
      <description></description>
    </item>
    
    <item>
      <title>HealthPlot</title>
      <link>/project/healthplot/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/healthplot/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rsciencexplorer</title>
      <link>/project/rsciencexplorer/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/rsciencexplorer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ScienceNet</title>
      <link>/project/sciencenet/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/sciencenet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NOMePlot - analysis of DNA methylation and nucleosome occupancy at the single molecule</title>
      <link>/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/journal-article/</guid>
      <description>&lt;!---
# &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;h1 id=&#34;click-the-cite-button-above-to-demo-the-feature-to-enable-visitors-to-import-publication-metadata-into-their-reference-management-software&#34;&gt;Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.&lt;/h1&gt;
&lt;h1 id=&#34;heading&#34;&gt;&lt;/h1&gt;

  &lt;/div&gt;
&lt;/div&gt;


# &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;h1 id=&#34;click-the-slides-button-above-to-demo-academics-markdown-slides-feature&#34;&gt;Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.&lt;/h1&gt;
&lt;h1 id=&#34;heading&#34;&gt;&lt;/h1&gt;

  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/).
--&gt;&lt;blockquote&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
